
O termo redes neurais profundas, ou deep learning, se refere a redes neurais artificiais com múltiplas camadas ocultas.Foram uma das principais tecnologias de aprendizado de máquina desenvolvidas nos últimos anos, e se tornaram cada vez mais popular. Devido a sua superior performance em extração de características, teve sucesso por distintos domínios, como visão computacional, reconhecimento de fala, processamento natural de linguagem e em big data. Uma arquitetura clássica é a rede neural convolucional (CNN), que utiliza convoluções para extrair características de uma imagem entre cada camada de filtros. Também possui camadas de pooling, não lineares e camadas completamente conectadas \cite{8308186}. Uma dos pressupostos das CNNs é que os filtros são indiferentes a translações das características na imagem, possibilitando assim uma eficiente extração de características para composição e identificação da imagem.


Pilotagem tripulada consiste, além da leitura dos instrumentos, em referências visuais, como ponto de referencias, terrenos, e marcações. Apesar dos instrumentos de leitura, a pilotagem tripulada é mais eficiente quando se utiliza de referências visuais, por se tratar de uma alternativa às falhas dos instrumentos. A navegação visual é uma tarefa complexa de ser implementada autonomamente, pois requer identificação de características do terreno em diversas condições e contextos. Em \cite{COUTURIER2021103666} é apresentado duas formas abordagens de navegação visual. A primeira citada é a localização visual relativa (RLV), que tem como objetivo estimar a posição baseado no deslocamento de um frame para o outro. A segunda abordagem é a localização visual absoluta, que busca comparar as imagens do VANT ou aeronave com imagens georeferenciadas. A segunda alternativa é interessante por se tratar de imagens rotuladas com as referencias geográficas e por serem referencias absolutas, que não sofrem de drift em relação a um referencial relativo. Para implementar a AVL, uma das possíveis abordagens é utilizando CNNs. Mesmo que os custos computacionais para treinar uma rede sejam altos, a utilização do modelo em um VANT pode ter custos reduzidos, que é um aspecto vantajoso, já que estes possuem recursos computacionais reduzidos.

modelos bons de dnn pra localização 


problema de labeling, aprendizado semi supervisionado

topologias de dnn 

hardware e factivel disponivel para micro aeronaves 

trabalhos relacionados 



\cite{rs13194017}

Principais tipos de localização
odometria visual, feature points, deep learning, template matching, encoder
semantic segmentation, representation space

Problemas: 
  rotulação
   solução: transfer learning, segmentação semantica 


\section{\textit{Compressão de DNNs}}\label{sec:Cap2_MR}


Redes neurais profundas estão no estado da arte em aplicações de visão computacional, à nivel de capacidade humana, contudo ao custo da faixa de dezenas a centenas de milhões de parâmetros e complexidade computacional. Também são dependem muito tempo e energia para treinar, e vasto conjunto de dados de treino. Essas grandes DNNs são difíceis de serem implementadas em ambientes embarcados, devido a fatores limitantes como banda, instruções por segundo e memória. 

Para lidar com problema de limitações de recursos, uma abordagem possível é a técnica de podagem de redes neurais profundas, que remove as neurônios que não são relevantes para a rede. Como é citado em \cite{jordao2019pruning}, foi possível reduzir até 67\% das operações de ponto flutuante (FLOPs), sem perda na acurácia e também foi possível reduzir 90\% do custo com uma perda negligente. Neste trabalho foi possível melhorar a acurácia comparado ao original, devido a regularização da rede. Consiste em estimar a importância do neurônio baseado em sua relação com a classe em um espaço dimensional reduzido. A relação é computada utilizando mínimos quadrados parciais e importância de variável em projeção. Em \cite{jordao2019pruning} também são mencionadas outras importante técnicas de compressão de DNNs, como quantização de float, que consiste da redução de float de 32 bits pata 8 bits, nos pesos e funções de ativação, reduzindo a precisão, acelerando as operações atomicas, contudo sem impacto relevante na acurácia do modelo. Aceleração de hardware também possibilita operações atomicas de multiplicações matriciais e altamente paralelizadas. 


