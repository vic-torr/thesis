

\section{\textit{Domínio do problema}}\label{sec:Cap2_dominio}

Pilotagem tripulada consiste, além da leitura dos instrumentos, em referências visuais, como ponto de referências, terrenos, e marcações. Apesar de altamente dependente dos instrumentos, a pilotagem tripulada é mais eficiente quando se utiliza de referências visuais, por se tratar de uma alternativa às falhas dos instrumentos. A navegação visual é uma tarefa complexa de ser implementada autonomamente, pois requer identificação de características do terreno em diversas condições e contextos~\cite{rs13194017}. Na revisão literária de~\cite{COUTURIER2021103666} é apresentado duas abordagens de localizão visual. A primeira citada é a localização visual relativa (\textit{RLV}\footnote{Relative visual location}), cujo objetivo é estimar a posição baseado no deslocamento de um quadro para o outro. A segunda abordagem é a localização visual absoluta (\textit{AVL}\footnote{Absolute visual location}), que compara as imagens do VANT ou aeronave com imagens georreferenciadas. A segunda alternativa é interessante por se tratar de imagens rotuladas com as referências geográficas e por serem referencias absolutas, que não sofrem de \textit{drift} em relação a um referencial relativo, porém com menor precisão. 


\section{\textit{Breve definição de redes neurais e convolucionais}}\label{sec:Cap2_definicoes}

O termo redes neurais embarca uma grande classe de modelos e métodos de aprendizados. O modelo mais simples, também podendo ser chamado de de rede de camada oculta única de percéptrons. Já o  percéptron é a célula de uma rede neural. Se trata de um modelo matemático análogo a um neurônio. Possui um vetor de entradas e sobre essas entradas é aplicada uma combinação linear utilizando os pesos sobre cada entrada. O resultado de tal operação por sua vez passa por uma função de ativação que resulta numa saída binária de classificação do percéptron. Dessa forma, ao se otimizar os pesos e função de ativação a determinadas amostras de treino e suas respectivas saídas rotuladas, podemos criar um classificador linear simples, caso seja um problema linearmente separável. Portanto o processo de treinamento de uma rede neural se trata de otimizar os pesos dos neurônios. Uma etapa importante dos ajustes dos pesos é a retropropagação de erro \footnote{Backpropagation} que é um algoritmo que otimiza os pesos da camada oculta~\cite{hastie01statisticallearning}. 

O termo redes neurais profundas, ou \textit{deep learning}, se refere a redes neurais artificiais com múltiplas camadas ocultas.Foram uma das principais tecnologias de aprendizado de máquina desenvolvidas nos últimos anos, e se tornaram cada vez mais popular. Devido a sua superior \textit{performance} em extração de características, teve sucesso por distintos domínios, como visão computacional, reconhecimento de fala, processamento natural de linguagem e em big data. Uma arquitetura clássica é a rede neural convolucional (CNN), que utiliza convoluções para extrair características de uma imagem entre cada camada de filtros. Também possui camadas de \textit{pooling}, não lineares e camadas completamente conectadas~\cite{8308186}. Uma dos pressupostos das \textit{CNNs} é os filtros serem indiferentes a translações das características na imagem, possibilitando assim uma eficiente extração de características para composição e identificação da imagem.

Um dos riscos envolvendo o treinamento de redes neurais profundas é o problema de \textit{overfiting}\footnote{Sobre-ajuste}. Se trata de quando o modelo é treinado e de forma a gera uma função proxima demais aos dados de treino, e que perdem generalidade, falhando em predições em dados fora do conjunto de treino. Para mitigar o surgimento de \textit{overfiting} durante o treino, são utilizadas técnicas de regularização. Consistem em adicionar penalidade à complexidade do modelo, de forma que o treino otimize para se tornar uma função genérica. Dentre as técnicas de regularização possíveis de DNNs, podemos citar o Dropout, Dropconnect e pruning, que são respectivamente a remoção, adição de coneção entre neurônios e remorção de neurônios~\cite{hastie01statisticallearning}.

\section{\textit{O problema de rotulagem e variabilidade de amostras de treino}}\label{sec:Cap2_rotulagem}


Um dos principais desafios envolvido o treino de \textit{CNNs} é representar um estado de características que cubram as variações fotográficas, tanto em características do sensor, como variações da imagem no dia, clima, estação e plataforma da câmera o que se torna uma tarefa difícil. Para uma localização efetiva, o modelo deve ser robusto a todas essas variações, que requer um grande conjunto de treino que cobre boa parte das diversas condições possíveis. Tal conjunto de dados não é disponível e nem viável de obter, pois se trata um volume muito grande de amostras~\cite{rs13194017}. Também não é viável o aprendizado contínuo em tempo real, dado o alto custo de poder computacional e tempo. Tais limitações levam a necessitar o desenvolvimento de algorítimos que aprendem seletivamente para que o poder computacional seja utilizado eficientemente, bem como reutilizar conhecimento prévio e evitar treinamento redundante~\cite{rostami2019learning}.  Dentre as técnicas utilizadas para implementar esses modelos mais eficientes, temos como exemplo o aprendizado supervisionado fraco, e a transferência de aprendizado. 

\section{\textit{Aprendizado semi-supervisionado}}\label{sec:Cap2_semisup}

As técnicas de aprendizado semi-supervisionado consistem em treinar um modelo com apenas um conjunto reduzido de amostras rotuladas de treino, e as demais amostras serem não supervisionadas. As demais amostras de treino podem ser utilizadas, por exemplo, agrupando-as com as amostras rotuladas e classificando-as como a amostra mais próxima, como apresenta o trabalho de~\cite{Sanches2003}. Outras propostas envolvem data augumentation, que consistem em gerar um conjunto de treino maior, dados as amostras de treino disponíveis.

\section{\textit{Transferência de aprendizado}}\label{sec:Cap2_transfer}

Já técnicas de transferência de aprendizado\footnote{Comumente citado como Transfer Learning}, ou \textit{few shots learning}, consistem em redes treinadas para um conjunto limitado de testes~\cite{rostami2019learning}
e reutilizam esse conhecimento através de diferentes domínios, tarefas ou agentes. Consistem primariamente de um problema origem e um problema objetivo e como podemos suceder em transferir conhecimento dado o problema origem. A abordagem de transferência de conhecimento se inspira em replicar a habilidade dos humanos em que é possível transferir conhecimento de experiências passadas para lidar com tarefas com poucas amostras rotuladas. Este fato inspirou em representar dados de diferentes problemas de aprendizado de máquina em um espaço embutido onde as representações utilizam de diversas relações entre diferentes domínios de conhecimentos e tarefas. Uma implementação encontrada na literatura, proposta em~\cite{DBLP:journals/corr/abs-1811-04863}, consistiu de transplantar a camada de características de uma CNN derivada do domínio de origem para inicializar outra rede, do domínio objetivo, composta por uma camada final fortemente conectada. Assim foi aproveitada as primeiras camadas e a rede foi trenada para o domínio objetivo com uma quantidade menor de amostras.

\section{\textit{Soluções em outros trabalhos}}\label{sec:Cap2_propostas}


Em~\cite{COUTURIER2021103666} são citadas soluções que envolvem a técnica \textit{template matching}. Contudo, essa solução envolve um alto custo para se realizar uma busca em uma área grande, portanto sendo necessário a implementação de técnicas como janelas deslizantes. 

Em~\cite{9552597} é implementado um modelo não supervisionado de segmentação semântica de terreno por imagens aéreas, com a vantagem de não depender de rotulagem, em comparação com outras técnicas de redes profundas.

Para implementar a \textit{AVL}, uma das possíveis abordagens é utilizando \textit{CNNs}, como é explorado em \cite{rs13194017}, que realizou um benchmark de modelos. Uma conclusão relevante foi que os custos computacionais para treinar uma rede sejam altos, a utilização do modelo em um VANT pode ter custos reduzidos, sendo um aspecto vantajoso, já que estes possuem recursos computacionais reduzidos. O modelo obtido consumia 1-2GB de memória e com passadas de 10ms. Contudo o melhor modelo ainda não apresentou acurácia competitiva a trabalhos prévios.




\section{\textit{Otimizações de redes neurais}}\label{sec:Cap2_compressao}


Redes neurais profundas estão no estado da arte em aplicações de visão computacional, ao nível de capacidade humana, contudo ao custo da faixa de dezenas a centenas de milhões de parâmetros e complexidade computacional. Também são dependem muito tempo e energia para treinar, e vasto conjunto de dados de treino. Essas grandes DNNs são difíceis de serem implementadas em ambientes embarcados, devido a fatores limitantes como banda, instruções por segundo e memória. 

Para lidar com problema de limitações de recursos, uma abordagem possível é a técnica de podagem de redes neurais profundas, que remove as neurônios que não são relevantes para a rede. Como é citado em~\cite{jordao2019pruning}, foi possível reduzir até 67\% das operações de ponto flutuante (\textit{FLOPS}\footnote{Floating point operations per second }), sem perda na acurácia e também foi possível reduzir 90\% do custo com uma perda negligente. Neste trabalho foi possível melhorar a acurácia comparado ao original, devido à regularização da rede. Consiste em estimar a importância do neurônio baseado em sua relação com a classe em um espaço dimensional reduzido. A relação é computada utilizando mínimos quadrados parciais e importância de variável em projeção. Em~\cite{liang2021pruning} também são mencionadas outras importantes técnicas de compressão de DNNs, como quantização de ponto flutuante, que consiste da redução de 32 bits pata 8 bits, nos pesos e funções de ativação, reduzindo a precisão, acelerando as operações atômicas, contudo sem impacto relevante na acurácia do modelo. Aceleração de \textit{hardware} também possibilita operações atômicas de multiplicações matriciais e altamente paralelizadas. 

\section{\textit{ Considerações de implementações quanto a recursos computacionais}}\label{sec:Cap2_trabalhos}


Em \cite{jeon2021run} foram realizados \textit{benchmarks} de possíveis \textit{hardwares} para implementações de odometria visual utilizando câmeras \textit{stereo} e fusões sensoriais. Neste trabalho é possível concluir a alto ganho de desempenho devido à aceleração de \textit{hardware} dos módulos Nvidia Jetson, que utilizam unidades gráficas para acelerar o treino das \textit{CNNs}. Atendendo os requisitos de potência, consumindo apenas 7.5W e de tamanho, podendo ser portada em micro aeronaves.  Em \cite{Shiguemori2016Embedded} foi realizada a implementação uma AVL por rede neural, um Raspberry pi, que é um computador embarcado e possui mais recursos de processamento na \textit{CPU}\footnote{CPU - Central Process unit: unidade central de processamento} do que \textit{GPU}\footnote{GPU - Graphical Process unit: unidade gráfica de processamento}. Contudo  o que não garantiu desempenho suficiente para uma boa acurácia na localização visual. Portanto, para se portar um modelo em uma micro aeronave, dado as limitações de peso, potência e acurácia mínima é interessante utilizar sistemas embarcados que possuem aceleração de \textit{hardware}, especializada em computação gráfica, como os módulos Jetson da NVIDIA.



